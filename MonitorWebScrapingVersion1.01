'''

Python web scraper for Newegg for Monitors

'''


import csv
import requests
from bs4 import BeautifulSoup


#import xlwt

#pythonWebScrapingGPUS = Workbook()

def csv_table_OverWrite(filename, data):
    """
    Creates a CSV file with the given filename, headers, and data.

    Args:
        filename (str): The name of the CSV file to create.
        headers (list): A list of strings representing the column headers.
        data (list of lists): A list of lists, where each inner list represents a row of data.
    """
    with open(filename, 'w', newline='') as csvfile:
        fieldnames = ['Description', 'CurrentPrice', 'PreviousPrice', 'DiscountedAmount', 'Availability', 'Reviews', 'URL', 'Brand']
        writer = csv.DictWriter(csvfile, delimiter='|', fieldnames=fieldnames)
        
        writer.writeheader()
        writer.writerows(data)

#this loop iterates through the each page of the website
url1 = 'https://www.newegg.com/Gaming-Monitor/SubCategory/ID-3743/Page-1?PageSize=96&Order=0'

pageNumber = 1

#writing object headers to csv filename
filename = 'webscrapingGPUData'
#headers of the html stie
headers = ['Description', 'CurrentPrice', 'PreviousPrice', 'DiscountedAmount', 'Availability', 'Reviews', 'URL', 'Brand' ]
#establishes the array for
itemDictionary = [ {'Description':'NULL', 'CurrentPrice':'NULL', 'PreviousPrice':'NULL', 'DiscountedAmount':'NULL', 'Availability':'NULL', 'Reviews':'NULL', 'URL':'NULL', 'Brand':'NULL' } ]

while pageNumber < 2:
    
    
    
    try:
        searchPage = requests.get(url1)
    except:
        print('ERROR: URL COULD NOT BE FOUND')

    searchPageSoup = BeautifulSoup(searchPage.text, 'html.parser')
    itemObjects = searchPageSoup.find_all('div', class_='item-cell')
    
    print(url1)

    for itemObject in itemObjects:
        
        #checks the product description
        try:
            Description = itemObject.find('a', class_='item-title').text
        except:
            Description = 'NULL'
            
        #checks the current product price
        try:
            
            CurrentPrice = itemObject.find('li', class_='price-current').find('strong').text
        except:
            CurrentPrice = 'NULL'
            
        #checks the preivous product price
        try:
            PreviousPrice = itemObject.find('li', class_='price-was-data').find('strong').text
        except:
            PreviousPrice = 'NULL'
            
        #checks the discount of product
        try:
            DiscountedAmount = itemObject.find('li', class_='price-save-percent').find('strong').text
        except:
            DiscountedAmount = 'NULL'
        
        #checks if product is in stock
        try:
            Availability = itemObject.find('p', class_='item-promo').text
        except:
            Availability = 'IN STOCK'

        #checks amount of ratings product has   
        try:
            Reviews = itemObject.find('span', class_='item-rating-num').text
        except:
            Reviews = 'NULL'
            
        #grabs the product page URL for the product  
        try:
            URL = itemObject.find('a', class_='item-title').get('href')
        except:
            URL = 'NULL'

        
        #now with the product page URL we will scrape that to get more specific data such as specs of the product 'Brand', 'Model', 'ScreenSize', 'Panel'
        try:
            productPage = requests.get(URL)
        except:
            print('ERROR: Product Page URL COULD NOT BE FOUND')

        productPageSoup = BeautifulSoup(productPage.text, 'html.parser')
        
        #grabs Brand
        try:
            Brand = productPageSoup.find('div', class_='table-horizontal').find('th', 'Brand').text
        except:
            Brand = 'NULL'
            
        itemDictionary.append( {'Description': Description, 'CurrentPrice': CurrentPrice, 'PreviousPrice': PreviousPrice, 'DiscountedAmount':DiscountedAmount, 'Availability': Availability, 'Reviews': Reviews, 'URL':URL, 'Brand':Brand } )
    
    print(pageNumber)
    
    #changes url to get the next page
    StringPageNumberPrev = 'Page-' + str(pageNumber)
    pageNumber+=1
    StringPageNumberNext = 'Page-' + str(pageNumber)   
    url1 = url1.replace(StringPageNumberPrev, StringPageNumberNext)
    
    
csv_table_OverWrite(filename, itemDictionary)
    


    
    
