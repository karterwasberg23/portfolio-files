
'''

Python web scraper for Newegg for Monitors

'''

import time
import csv
import requests
from bs4 import BeautifulSoup


#import xlwt

#pythonWebScrapingGPUS = Workbook()

def csv_table_OverWrite(filename, data):
    """
    Creates a CSV file with the given filename, headers, and data.

    Args:
        filename (str): The name of the CSV file to create.
        headers (list): A list of strings representing the column headers.
        data (list of lists): A list of lists, where each inner list represents a row of data.
    """
    with open(filename, 'w', newline='') as csvfile:
        fieldnames = ['Description', 'CurrentPrice', 'PreviousPrice', 'DiscountedAmount', 'Availability', 'Reviews', 'URL', 'Brand', 'Model', 'ScreenSize', 'Panel', 'RecommendedResolution', 'AspectRatio', 'ResponseTime']
        writer = csv.DictWriter(csvfile, delimiter='|', fieldnames=fieldnames)
        
        writer.writeheader()
        writer.writerows(data)

#this loop iterates through the each page of the website
url1 = 'https://www.newegg.com/Gaming-Monitor/SubCategory/ID-3743/Page-1?PageSize=96&Order=0'

pageNumber = 1

#writing object headers to csv filename
filename = 'webscrapingGPUData'
#headers of the html stie
headers = ['Description', 'CurrentPrice', 'PreviousPrice', 'DiscountedAmount', 'Availability', 'Reviews', 'URL', 'Brand', 'Model', 'ScreenSize', 'Panel', 'RecommendedResolution', 'AspectRatio', 'ResponseTime' ]
#establishes the array for
itemDictionary = [ {'Description':'NULL', 'CurrentPrice':'NULL', 'PreviousPrice':'NULL', 'DiscountedAmount':'NULL', 'Availability':'NULL', 'Reviews':'NULL', 'URL':'NULL', 'Brand':'NULL', 'Model':'NULL', 'ScreenSize':'NULL', 'Panel':'NULL', 'RecommendedResolution':'NULL', 'AspectRatio':'NULL', 'ResponseTime':'NULL' } ]

while pageNumber < 2:
    
    grabProducts = False
    while grabProducts==False:
        searchPage = requests.get(url1)
        searchPageSoup = BeautifulSoup(searchPage.text, 'html.parser')
        itemObjects = searchPageSoup.find_all('div', class_='item-cell')
        
        if len(itemObjects)>1:
            grabProducts = True
        else:
            print('ERROR: URL COULD NOT BE FOUND')

    
    
    print(url1)
    
    itemNumber = 0
    for itemObject in itemObjects:
        
        #checks the product description
        try:
            Description = itemObject.find('a', class_='item-title').text
        except:
            Description = 'NULL'
            
        #checks the current product price
        try:
            
            CurrentPrice = itemObject.find('li', class_='price-current').find('strong').text
        except:
            CurrentPrice = 'NULL'
            
        #checks the preivous product price
        try:
            PreviousPrice = itemObject.find('li', class_='price-was-data').find('strong').text
        except:
            PreviousPrice = 'NULL'
            
        #checks the discount of product
        try:
            DiscountedAmount = itemObject.find('li', class_='price-save-percent').find('strong').text
        except:
            DiscountedAmount = 'NULL'
        
        #checks if product is in stock
        try:
            Availability = itemObject.find('p', class_='item-promo').text
        except:
            Availability = 'IN STOCK'

        #checks amount of ratings product has   
        try:
            Reviews = itemObject.find('span', class_='item-rating-num').text
        except:
            Reviews = 'NULL'
            
        #grabs the product page URL for the product  
        try:
            URL = itemObject.find('a', class_='item-title').get('href')
        except:
            URL = 'NULL'

        
        #now with the product page URL we will scrape that to get more specific data such as specs of the product 'Brand', 'Model', 'ScreenSize', 'Panel', 'RecommendedResolution', 'AspectRatio', 'ResponseTime'
        foundProductPage = False
        
        foundProductLoop = 0
        while foundProductPage==False:
            productPage = requests.get(URL)
            productPageSoup = BeautifulSoup(productPage.text, 'html.parser')
            productSpecsTable = productPageSoup.find('div', class_='tab-panes')
            if productSpecsTable!=None:
                foundProductPage = True
                print('Found product page: '+URL)
                invalidURL = 'NULL'
            else:
                invalidURL = URL
                print('Invalid Url: Trial#: ' + str(foundProductLoop) + 'URL: ' + invalidURL)
                foundProductLoop += 1
        
        #print('product looped:' + str(foundProductLoop) + 'times')
        
        #grabs Brand from product page
        try:
            Brand = (tabPanesDiv
                .find(lambda tag: tag.name == 'th' and 'Brand' in tag.text).find_next_sibling('td').text)
        except:
            Brand = 'NULL'
            
        #grabs Model from product page
        try:
            Model = (tabPanesDiv
                .find(lambda tag: tag.name == 'th' and 'Model' in tag.text).find_next_sibling('td').text)
        except:
            Model = 'NULL'
            
        #grabs ScreenSize from product page
        try:
            ScreenSize = (tabPanesDiv
                .find(lambda tag: tag.name == 'th' and 'Screen Size' in tag.text).find_next_sibling('td').text)
        except:
            ScreenSize = 'NULL'
        
        #grabs Panel from product page
        try:
            Panel = (tabPanesDiv
                .find(lambda tag: tag.name == 'th' and 'Panel' in tag.text).find_next_sibling('td').text)
        except:
            Panel = 'NULL'
            
        #grabs RecommendedResolution from product page
        try:
            RecommendedResolution = (tabPanesDiv
                .find(lambda tag: tag.name == 'th' and 'Recommended Resolution' in tag.text).find_next_sibling('td').text)
        except:
            RecommendedResolution = 'NULL'
            
        #grabs AspectRatio from product page
        try:
            AspectRatio = (tabPanesDiv
                .find(lambda tag: tag.name == 'th' and 'Aspect Ratio' in tag.text).find_next_sibling('td').text)
        except:
            AspectRatio = 'NULL'
        
        #grabs ResponseTime from product page
        try:
            ResponseTime = (tabPanesDiv
                .find(lambda tag: tag.name == 'th' and 'Response Time' in tag.text).find_next_sibling('td').text)
        except:
            ResponseTime = 'NULL'
        
        itemDictionary.append( {'Description': Description, 'CurrentPrice': CurrentPrice, 'PreviousPrice': PreviousPrice, 'DiscountedAmount':DiscountedAmount, 'Availability': Availability, 'Reviews': Reviews, 'URL':URL, 'Brand':Brand, 'Model':Model, 'ScreenSize':ScreenSize, 'Panel':Panel, 'RecommendedResolution':RecommendedResolution, 'AspectRatio':AspectRatio, 'ResponseTime':ResponseTime } )
        
        #print('success item scrape on Page: ' + str(pageNumber) + ' Item: ' + str(itemNumber) )
        itemNumber+=1
        
    print(pageNumber)
    
    #changes url to get the next page
    StringPageNumberPrev = 'Page-' + str(pageNumber)
    pageNumber+=1
    StringPageNumberNext = 'Page-' + str(pageNumber)   
    url1 = url1.replace(StringPageNumberPrev, StringPageNumberNext)
    
    
csv_table_OverWrite(filename, itemDictionary)
    


    
    
