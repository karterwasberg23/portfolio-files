'''

python GPU webscraper for newegg

'''


import csv
import requests
from bs4 import BeautifulSoup




#import xlwt

#pythonWebScrapingGPUS = Workbook()

def csv_table_OverWrite(filename, data):
    """
    Creates a CSV file with the given filename, headers, and data.

    Args:
        filename (str): The name of the CSV file to create.
        headers (list): A list of strings representing the column headers.
        data (list of lists): A list of lists, where each inner list represents a row of data.
    """
    with open(filename, 'w', newline='') as csvfile:
        fieldnames = ['Description', 'Brand', 'CurrentPrice', 'PreviousPrice', 'Availability', 'Reviews']
        writer = csv.DictWriter(csvfile, delimiter='|', fieldnames=fieldnames)
        
        writer.writeheader()
        writer.writerows(data)

#this loop iterates through the each page of the website
url1 = 'https://www.newegg.com/GPUs-Video-Graphics-Cards/SubCategory/ID-48/Page-1?PageSize=96'

pageNumber = 1

#writing object headers to csv filename
filename = 'webscrapingGPUData'
headers = ['Description', 'Brand', 'CurrentPrice', 'PreviousPrice', 'Availability', 'Reviews']
itemDictionary = [ {'Description': 'NULL', 'Brand': 'NULL', 'CurrentPrice':'NULL', 'PreviousPrice':'NULL', 'Availability': 'NULL', 'Reviews': 'NULL' } ]

while pageNumber < 21:
    
    
    GrabProductsPageNum = 0
    while GrabProductsPageNum < 5:
        searchPage = requests.get(url1)
        searchPageSoup = BeautifulSoup(searchPage.content, 'html.parser')
        itemObjects = searchPageSoup.find_all('div', class_='item-cell')
        
        if len(itemObjects) > 3:
            print('successful scrape on page: ' + str(pageNumber) + ' URL: ' + url1)
            break
        
        if GrabProductsPageNum >= 4:
            print('Unable to parse page: ' + str(pageNumber) + ' URL: ' + url1)
            
        GrabProductsPageNum+=1

    for itemObject in itemObjects:
        
        
        #checks the product description
        try:
            Description = itemObject.find('a', class_='item-title').text
        except:
            Description = 'NULL'
            
        #checks if product is in stock
        try:
            Availability = itemObject.find('p', class_='item-promo').text
        except:
            Availability = 'IN STOCK'
        
        #checks the brand of product
        try:
            brandImage = itemObject.find('a', class_='item-brand').find('img')
            Brand = brandImage['alt']
        except:
            Brand = 'NULL'
        
        #checks the current product price
        try:
            
            CurrentPrice = itemObject.find('li', class_='price-current').find('strong').text
        except:
            CurrentPrice = 'NULL'
            
        #checks the preivous product price
        try:
            PreviousPrice = itemObject.find('li', class_='price-was-data').find('strong').text
        except:
            PreviousPrice = 'NULL'

        #checks amount of ratings product has   
        try:
            Reviews = itemObject.find('span', class_='item-rating-num').text
        except:
            Reviews = 'NULL'
            
            
        itemDictionary.append( {'Description': Description, 'Brand': Brand, 'CurrentPrice':CurrentPrice, 'PreviousPrice':PreviousPrice, 'Availability': Availability, 'Reviews': Reviews } )
    
    
    #changes url to get the next page
    StringPageNumberPrev = 'Page-' + str(pageNumber)
    pageNumber+=1
    StringPageNumberNext = 'Page-' + str(pageNumber)   
    url1 = url1.replace(StringPageNumberPrev, StringPageNumberNext)


csv_table_OverWrite(filename, itemDictionary)
    


    
    
